
@article{decarlo_optical_nodate,
	title = {Optical {Flow} {Constraints} on {Deformable} {Models} with {Applications} to {Face} {Tracking}},
	abstract = {Optical ﬂow provides a constraint on the motion of a deformable model. We derive and solve a dynamic system incorporating ﬂow as a hard constraint, producing a model-based least-squares optical ﬂow solution. Our solution also ensures the constraint remains satisﬁed when combined with edge information, which helps combat tracking error accumulation. Constraint enforcement can be relaxed using a Kalman ﬁlter, which permits controlled constraint violations based on the noise present in the optical ﬂow information, and enables optical ﬂow and edge information to be combined more robustly and efﬁciently. We apply this framework to the estimation of face shape and motion using a 3D deformable face model. This model uses a small number of parameters to describe a rich variety of face shapes and facial expressions. We present experiments in extracting the shape and motion of a face from image sequences which validate the accuracy of the method. They also demonstrate that our treatment of optical ﬂow as a hard constraint, as well as our use of a Kalman ﬁlter to reconcile these constraints with the uncertainty in the optical ﬂow, are vital for improving the performance of our system.},
	language = {en},
	author = {Decarlo, Douglas and Metaxas, Dimitris},
	pages = {29},
	file = {Decarlo und Metaxas - Optical Flow Constraints on Deformable Models with.pdf:C\:\\Users\\Florian\\Zotero\\storage\\MGNTYFBC\\Decarlo und Metaxas - Optical Flow Constraints on Deformable Models with.pdf:application/pdf},
}

@article{dorini_unscented_2011,
	title = {Unscented feature tracking},
	volume = {115},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314210001669},
	doi = {10.1016/j.cviu.2010.07.009},
	abstract = {Accurate feature tracking is the foundation of many high level tasks in computer vision, such as 3D reconstruction and motion analysis. Although there are many feature tracking algorithms, most of them do not maintain information about the error of the data being tracked. Also, due to the difficulty and spatial locality of the problem, existing methods can generate grossly incorrect correspondences, making outlier rejection an essential post-processing step. We propose a new generic framework that uses the Scaled Unscented Transform to augment arbitrary feature tracking algorithms, and use Gaussian Random Variables (GRV) for the representation of features’ locations uncertainties. We apply and validate the framework on the well-understood Kanade–Lucas–Tomasi feature tracker, and call it Unscented KLT (UKLT). The UKLT tracks GRVs and rejects incorrect correspondences, without a global model of motion. We validate our method on real and synthetic sequences, and demonstrate how the UKLT outperforms other approaches on both outlier rejection and the accuracy of feature locations.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Computer Vision and Image Understanding},
	author = {Dorini, Leyza Baldo and Goldenstein, Siome Klein},
	month = jan,
	year = {2011},
	keywords = {Feature tracking, Outlier rejection, Statistical correspondences, Uncertainty tracking},
	pages = {8--15},
	file = {ScienceDirect Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\JKZ7KDBQ\\S1077314210001669.html:text/html;Dorini und Goldenstein - 2011 - Unscented feature tracking.pdf:C\:\\Users\\Florian\\Zotero\\storage\\QX3LXECL\\Dorini und Goldenstein - 2011 - Unscented feature tracking.pdf:application/pdf},
}

@inproceedings{williams_extended_1998,
	title = {An extended {Kalman} filtering approach to high precision stereo image matching},
	volume = {2},
	doi = {10.1109/ICIP.1998.723337},
	abstract = {We present a novel approach to stereo image matching for high precision applications which is based upon a non-linear filtering technique called the extended Kalman filter (EKF). The matching algorithm has three components-the matching process, false match rejection, and disparity prediction, which are all derived within the Kalman filtering framework. We first present the stereo matching model used, and then derive the matching equations and processes. We then present results of tests performed on a synthetic stereo pair, which allows comparison with ground truth data. The results indicate that the method is capable of very robust and high precision matching performance.},
	booktitle = {Proceedings 1998 {International} {Conference} on {Image} {Processing}. {ICIP98} ({Cat}. {No}.{98CB36269})},
	author = {Williams, J. and Bennamoun, M.},
	month = oct,
	year = {1998},
	keywords = {Testing, Space technology, Equations, Filtering, Image matching, Kalman filters, Least squares methods, Matched filters, Robustness, Satellites},
	pages = {157--161 vol.2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\7TG2KI2A\\723337.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\WL6F7477\\Williams und Bennamoun - 1998 - An extended Kalman filtering approach to high prec.pdf:application/pdf},
}

@inproceedings{tommasini_robust_1998,
	title = {Robust feature tracking in underwater video sequences},
	volume = {1},
	doi = {10.1109/OCEANS.1998.725642},
	abstract = {The paper proposes a robust feature tracker based on an efficient outlier rejection scheme, suitable for feature tracking in subsea video sequences. We extend the Shi-Tomasi-Kanade scheme (C. Tomasi and T. Kanade, 1991; J. Shi and C. Tomasi, 1994) by introducing a technique for rejecting spurious features. We employ a simple and efficient outlier rejection rule, called X84, and prove that its theoretical assumptions are satisfied in the feature tracking scenario. Experiments with synthetic and real subsea sequence confirm that our algorithm locates and discards unreliable features accurately and consistently, and tracks good features reliably over many frames. We also illustrate quantitatively the benefits introduced by the algorithm with the example of fundamental matrix estimation.},
	booktitle = {{IEEE} {Oceanic} {Engineering} {Society}. {OCEANS}'98. {Conference} {Proceedings} ({Cat}. {No}.{98CH36259})},
	author = {Tommasini, T. and Fusiello, A. and Roberto, V. and Trucco, E.},
	month = sep,
	year = {1998},
	keywords = {Laboratories, Robustness, Computer vision, Image sequences, Machine vision, Oceans, Optical filters, Stereo vision, Underwater tracking, Video sequences},
	pages = {46--50 vol.1},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\NTFWW4CK\\Tommasini et al. - 1998 - Robust feature tracking in underwater video sequen.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\3DV6H4NH\\725642.html:text/html},
}

@inproceedings{tommasini_making_1998,
	title = {Making good features track better},
	doi = {10.1109/CVPR.1998.698606},
	abstract = {This paper addresses robust feature tracking. We extend the well-known Shi-Tomasi-Kanade tracker by introducing an automatic scheme for rejecting spurious features. We employ a simple and efficient outlier rejection rule, called X84, and prove that its theoretical assumptions are satisfied in the feature tracking scenario. Experiments with real and synthetic images confirm that our algorithm makes good features track better; we show a quantitative example of the benefits introduced by the algorithm for the case of fundamental matrix estimation. The complete code of the robust tracker is available via ftp.},
	booktitle = {Proceedings. 1998 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({Cat}. {No}.{98CB36231})},
	author = {Tommasini, T. and Fusiello, A. and Trucco, E. and Roberto, V.},
	month = jun,
	year = {1998},
	note = {ISSN: 1063-6919},
	keywords = {Laboratories, Kalman filters, Computer vision, Image sequences, Machine vision, Optical filters, Stereo vision, Electronic switching systems, Informatics, Time frequency analysis},
	pages = {178--183},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\DL4PPSJ8\\Tommasini et al. - 1998 - Making good features track better.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\FWLCSHGG\\698606.html:text/html},
}

@techreport{pareek_evaluation_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Evaluation of {Feature} {Detector}-{Descriptor} {Using} {RANSAC} for {Visual} {Tracking}},
	url = {https://papers.ssrn.com/abstract=3354470},
	abstract = {Vision-based tracking is an essential prerequisite to a growing number of applications in computer vision. Any tracking algorithm is expected to deal with problems like a change in intensity, scale, pose, and camera motion. This paper summarizes the implementation of such algorithms stating their merits and demerits under various transformation and distortion of images like blurring, noise, intensity variation, and rotation. Also, implementation of an object recognition system is done that uses image matching techniques with RANSAC algorithm to identify the objects in a given scene using their 2-D images. AKAZE, BRISK, DAISY, FREAK, ORB, SIFT and, SURF algorithm has been used for feature detection, extraction, and matching. Then the outliers are removed by RANSAC algorithm and homography detects the object for each image matching algorithm.},
	language = {en},
	number = {ID 3354470},
	urldate = {2021-11-04},
	institution = {Social Science Research Network},
	author = {Pareek, Anshul and Arora, Nidhi},
	month = feb,
	year = {2019},
	doi = {10.2139/ssrn.3354470},
	keywords = {Anshul Pareek, Evaluation of Feature Detector-Descriptor Using RANSAC for Visual Tracking, Nidhi Arora, SSRN},
	file = {Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\U6JT4XYR\\Pareek und Arora - 2019 - Evaluation of Feature Detector-Descriptor Using RA.pdf:application/pdf;Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\QXBICLHD\\papers.html:text/html},
}

@article{fusiello_improving_1999,
	title = {Improving {Feature} {Tracking} with {Robust} {Statistics}},
	volume = {2},
	issn = {1433-7541},
	url = {https://doi.org/10.1007/s100440050039},
	doi = {10.1007/s100440050039},
	abstract = {This paper addresses robust feature tracking. The aim is to track point features in a sequence of images and to identify unreliable features resulting from occlusions, perspective distortions and strong intensity changes. We extend the well-known Shi–Tomasi–Kanade tracker by introducing an automatic scheme for rejecting spurious features. We employ a simple and efficient outliers rejection rule, called X84, and prove that its theoretical assumptions are satisfied in the feature tracking scenario. Experiments with real and synthetic images confirm that our algorithm consistently discards unreliable features; we show a quantitative example of the benefits introduced by the algorithm for the case of fundamental matrix estimation. The complete code of the robust tracker is available via ftp.},
	language = {en},
	number = {4},
	urldate = {2021-11-04},
	journal = {Pattern Analysis \& Applications},
	author = {Fusiello, A. and Trucco, E. and Tommasini, T. and Roberto, V.},
	month = nov,
	year = {1999},
	pages = {312--320},
	file = {Springer Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\IS8P2CGF\\Fusiello et al. - 1999 - Improving Feature Tracking with Robust Statistics.pdf:application/pdf},
}

@inproceedings{songtao_improved_2017,
	title = {An improved method for eliminating false matches},
	doi = {10.1109/ICIVC.2017.7984533},
	abstract = {The Eliminating of false feature matches is an important follow-up work of feature detection extraction and matching, which is of great significance in improving the quality of image matching. In this paper, it experimentally analyzed several kinds of false feature matches rejection methods (Distance-ratio Criterion method, Bi-direction Matching method and RANSAC), which are performed based on the ORB feature point detection and descriptor extraction and the BF matching. In view of the overall filtering effect, RANSAC is relatively optimal, but it still has the disadvantages of slower filtering speed and fewer matching points after filtering. Propose an improved algorithm based on Bi-direction Matching method and Distance-ratio Criterion method, and conduct series of experiments with it. The experiments show that the filtering effect of the improved filtering algorithm is better than RANSAC. It is less time-consuming (reduced by 58.4\%, compared with RANSAC), and retains more correct matches (increased by 14.8\%, compared with RANSAC) while removing the false matches.},
	booktitle = {2017 2nd {International} {Conference} on {Image}, {Vision} and {Computing} ({ICIVC})},
	author = {Songtao, Zhang and Chao, Li and Liqing, Li},
	month = jun,
	year = {2017},
	keywords = {Feature extraction, Image matching, Matched filters, BF, bi-direction matching, Bidirectional control, distance-ratio criterion, Electronic mail, eliminating false matches, Filtering algorithms, ORB, RANSAC},
	pages = {133--137},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\ZBXM6VLT\\7984533.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\7STQMRNM\\Songtao et al. - 2017 - An improved method for eliminating false matches.pdf:application/pdf},
}

@article{cheng_improving_2019,
	title = {Improving monocular visual {SLAM} in dynamic environments: an optical-flow-based approach},
	volume = {33},
	issn = {0169-1864},
	shorttitle = {Improving monocular visual {SLAM} in dynamic environments},
	url = {https://doi.org/10.1080/01691864.2019.1610060},
	doi = {10.1080/01691864.2019.1610060},
	abstract = {Visual Simultaneous Localization and Mapping (visual SLAM) has attracted more and more researchers in recent decades and many state-of-the-art algorithms have been proposed with rather satisfactory performance in static scenarios. However, in dynamic scenarios, the performance of current visual SLAM algorithms degrades significantly due to the disturbance of the dynamic objects. To address this problem, we propose a novel method which uses optical flow to distinguish and eliminate the dynamic feature points from the extracted ones using RGB images as the only input. The static feature points are fed into the visual SLAM system for the camera pose estimation. We integrate our method with the original ORB-SLAM system and validate the proposed method with the challenging dynamic sequences from the TUM dataset and our recorded office dataset. The whole system can work in real time. Qualitative and quantitative evaluations demonstrate that our method significantly improves the performance of ORB-SLAM in dynamic scenarios.},
	number = {12},
	urldate = {2021-11-08},
	journal = {Advanced Robotics},
	author = {Cheng, Jiyu and Sun, Yuxiang and Meng, Max Q.-H.},
	month = jun,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01691864.2019.1610060},
	keywords = {dynamic environments, optical flow, Visual SLAM},
	pages = {576--589},
	file = {Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\TCSCNVLV\\Cheng et al. - 2019 - Improving monocular visual SLAM in dynamic environ.pdf:application/pdf;Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\EW5CZKMK\\01691864.2019.html:text/html},
}

@inproceedings{bang_camera_2017,
	title = {Camera {Pose} {Estimation} {Using} {Optical} {Flow} and {ORB} {Descriptor} in {SLAM}-{Based} {Mobile} {AR} {Game}},
	doi = {10.1109/PlatCon.2017.7883693},
	abstract = {A mobile augmented reality (AR) game application was developed based on simultaneous localization and mapping (SLAM). The SLAM-based AR game requires to estimate the pose from the camera input image in real time. Before running the game, point cloud data for a real-world game space is built. While the game is running, the camera pose is estimated by matching the prebuilt point cloud data and camera input image. To minimize errors in the matching, we present a hybrid method using an optical flow and ORB descriptor, where the optical flow accurately tracks the displacement of keypoints in consecutive images, and the ORB is a fast keypoint descriptor under a BSD license. The performance of the hybrid method is compared with a method using only the ORB descriptor matching. In addition, a mobile AR game embedding the hybrid method was tested in both indoor and outdoor environments.},
	booktitle = {2017 {International} {Conference} on {Platform} {Technology} and {Service} ({PlatCon})},
	author = {Bang, Junseong and Lee, Dongchun and Kim, Youngjun and Lee, Hunhoo},
	month = feb,
	year = {2017},
	keywords = {Cameras, Games, Computer vision, Adaptive optics, Image motion analysis, Optical imaging, Three-dimensional displays},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\7E6EPZH8\\7883693.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\W562XLVA\\Bang et al. - 2017 - Camera Pose Estimation Using Optical Flow and ORB .pdf:application/pdf},
}

@inproceedings{buemi_orb-slam_2021,
	title = {{ORB}-{SLAM} {With} {Near}-{Infrared} {Images} and {Optical} {Flow} {Data}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/ACVR/html/Buemi_ORB-SLAM_With_Near-Infrared_Images_and_Optical_Flow_Data_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2021-11-05},
	author = {Buemi, Antonio and Bruna, Arcangelo and Petinot, Sylvain and Roux, Nicolas},
	year = {2021},
	pages = {1799--1804},
	file = {Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\99IUV3QX\\Buemi et al. - 2021 - ORB-SLAM With Near-Infrared Images and Optical Flo.pdf:application/pdf;Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\FZIF9WEW\\Buemi_ORB-SLAM_With_Near-Infrared_Images_and_Optical_Flow_Data_ICCVW_2021_paper.html:text/html},
}

@inproceedings{tareen_comparative_2018,
	title = {A comparative analysis of {SIFT}, {SURF}, {KAZE}, {AKAZE}, {ORB}, and {BRISK}},
	doi = {10.1109/ICOMET.2018.8346440},
	abstract = {Image registration is the process of matching, aligning and overlaying two or more images of a scene, which are captured from different viewpoints. It is extensively used in numerous vision based applications. Image registration has five main stages: Feature Detection and Description; Feature Matching; Outlier Rejection; Derivation of Transformation Function; and Image Reconstruction. Timing and accuracy of feature-based Image Registration mainly depend on computational efficiency and robustness of the selected feature-detector-descriptor, respectively. Therefore, choice of feature-detector-descriptor is a critical decision in feature-matching applications. This article presents a comprehensive comparison of SIFT, SURF, KAZE, AKAZE, ORB, and BRISK algorithms. It also elucidates a critical dilemma: Which algorithm is more invariant to scale, rotation and viewpoint changes? To investigate this problem, image matching has been performed with these features to match the scaled versions (5\% to 500\%), rotated versions (0° to 360°), and perspective-transformed versions of standard images with the original ones. Experiments have been conducted on diverse images taken from benchmark datasets: University of OXFORD, MATLAB, VLFeat, and OpenCV. Nearest-Neighbor-Distance-Ratio has been used as the feature-matching strategy while RANSAC has been applied for rejecting outliers and fitting the transformation models. Results are presented in terms of quantitative comparison, feature-detection-description time, feature-matching time, time of outlier-rejection and model fitting, repeatability, and error in recovered results as compared to the ground-truths. SIFT and BRISK are found to be the most accurate algorithms while ORB and BRISK are most efficient. The article comprises rich information that will be very useful for making important decisions in vision based applications and main aim of this work is to set a benchmark for researchers, regardless of any particular area.},
	booktitle = {2018 {International} {Conference} on {Computing}, {Mathematics} and {Engineering} {Technologies} ({iCoMET})},
	author = {Tareen, Shaharyar Ahmed Khan and Saleem, Zahra},
	month = mar,
	year = {2018},
	keywords = {ORB, RANSAC, affine invariance, AKAZE, BRISK, feature detection, feature matching, image matching, image registration, KAZE, mosaicing, nearest neighbor distance ratio, rotation invariance, scale invariance, SIFT, SURF},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\CRHV6KYV\\8346440.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\3S7EARZM\\Tareen und Saleem - 2018 - A comparative analysis of SIFT, SURF, KAZE, AKAZE,.pdf:application/pdf},
}

@article{wang_robust_2009,
	title = {Robust digital image stabilization using the {Kalman} filter},
	volume = {55},
	issn = {1558-4127},
	doi = {10.1109/TCE.2009.4814407},
	abstract = {In this paper, we present a new digital image stabilization (DIS) algorithm based on feature point tracking. Feature points well tracked by the Kanade-Lucas-Tomasi (KLT) tracker are used to estimate the global motion between two consecutive image frames. The motion prediction with the Kalman filter (KF) is incorporated into the KLT tracker to further speed up the tracking process. Moreover, we develop an adaptive KF to better handle the intentional motion of the camera. In addition, a new scheme is also proposed to detect the scene change and automatically change the reference frame. Experimental simulation shows that the proposed DIS algorithm has the characteristics of high accuracy, good robustness to different irregular conditions.},
	number = {1},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Wang, Chuntao and Kim, Jin-hyung and Byun, Keun-yung and Ni, Jiangqun and Ko, Sung-jea},
	month = feb,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Consumer Electronics},
	keywords = {Cameras, Robustness, Video sequences, adaptive Kalman filter, Computational complexity, Digital image stabilization (DIS), Digital images, feature point tracking, Karhunen-Loeve transforms, KLT tracker, Layout, Motion compensation, Motion estimation, scene change, Tracking},
	pages = {6--14},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\FKVMJ66P\\Wang et al. - 2009 - Robust digital image stabilization using the Kalma.pdf:application/pdf},
}

@article{civera_1-point_2010,
	title = {1-{Point} {RANSAC} for extended {Kalman} filtering: {Application} to real-time structure from motion and visual odometry},
	volume = {27},
	issn = {1556-4967},
	shorttitle = {1-{Point} {RANSAC} for extended {Kalman} filtering},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20345},
	doi = {10.1002/rob.20345},
	abstract = {Random sample consensus (RANSAC) has become one of the most successful techniques for robust estimation from a data set that may contain outliers. It works by constructing model hypotheses from random minimal data subsets and evaluating their validity from the support of the whole data. In this paper we present a novel combination of RANSAC plus extended Kalman filter (EKF) that uses the available prior probabilistic information from the EKF in the RANSAC model hypothesize stage. This allows the minimal sample size to be reduced to one, resulting in large computational savings without the loss of discriminative power. 1-Point RANSAC is shown to outperform both in accuracy and computational cost the joint compatibility branch and bound (JCBB) algorithm, a gold-standard technique for spurious rejection within the EKF framework. Two visual estimation scenarios are used in the experiments: first, six-degree-of-freedom (DOF) motion estimation from a monocular sequence (structure from motion). Here, a new method for benchmarking six-DOF visual estimation algorithms based on the use of high-resolution images is presented, validated, and used to show the superiority of 1-point RANSAC. Second, we demonstrate long-term robot trajectory estimation combining monocular vision and wheel odometry (visual odometry). Here, a comparison against global positioning system shows an accuracy comparable to state-of-the-art visual odometry methods. © 2010 Wiley Periodicals, Inc.},
	language = {en},
	number = {5},
	urldate = {2021-11-13},
	journal = {Journal of Field Robotics},
	author = {Civera, Javier and Grasa, Oscar G. and Davison, Andrew J. and Montiel, J. M. M.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20345},
	pages = {609--631},
	file = {Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\H2KPC8XG\\rob.html:text/html;Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\CV24P5UG\\Civera et al. - 2010 - 1-Point RANSAC for extended Kalman filtering Appl.pdf:application/pdf},
}

@article{chiella_quaternion-based_2019,
	title = {Quaternion-{Based} {Robust} {Attitude} {Estimation} {Using} an {Adaptive} {Unscented} {Kalman} {Filter}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/19/10/2372},
	doi = {10.3390/s19102372},
	abstract = {This paper presents the Quaternion-based Robust Adaptive Unscented Kalman Filter (QRAUKF) for attitude estimation. The proposed methodology modifies and extends the standard UKF equations to consistently accommodate the non-Euclidean algebra of unit quaternions and to add robustness to fast and slow variations in the measurement uncertainty. To deal with slow time-varying perturbations in the sensors, an adaptive strategy based on covariance matching that tunes the measurement covariance matrix online is used. Additionally, an outlier detector algorithm is adopted to identify abrupt changes in the UKF innovation, thus rejecting fast perturbations. Adaptation and outlier detection make the proposed algorithm robust to fast and slow perturbations such as external magnetic field interference and linear accelerations. Comparative experimental results that use an industrial manipulator robot as ground truth suggest that our method overcomes a trusted commercial solution and other widely used open source algorithms found in the literature.},
	language = {en},
	number = {10},
	urldate = {2021-11-13},
	journal = {Sensors},
	author = {Chiella, Antônio C. B. and Teixeira, Bruno O. S. and Pereira, Guilherme A. S.},
	month = jan,
	year = {2019},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive filtering, MARG sensor, unit quaternion, unscented Kalman filter},
	pages = {2372},
	file = {Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\LF5H4FEI\\2372.html:text/html;Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\S3I76D4E\\Chiella et al. - 2019 - Quaternion-Based Robust Attitude Estimation Using .pdf:application/pdf},
}

@article{chang_robust_2014,
	title = {Robust {Kalman} filtering based on {Mahalanobis} distance as outlier judging criterion},
	volume = {88},
	issn = {0949-7714, 1432-1394},
	url = {http://link.springer.com/10.1007/s00190-013-0690-8},
	doi = {10.1007/s00190-013-0690-8},
	abstract = {A robust Kalman ﬁlter scheme is proposed to resist the inﬂuence of the outliers in the observations. Two kinds of observation error are studied, i.e., the outliers in the actual observations and the heavy-tailed distribution of the observation noise. Either of the two kinds of errors can seriously degrade the performance of the standard Kalman ﬁlter. In the proposed method, a judging index is deﬁned as the square of the Mahalanobis distance from the observation to its prediction. By assuming that the observation is Gaussian distributed with the mean and covariance being the observation prediction and its associate covariance, the judging index should be Chi-square distributed with the dimension of the observation vector as the degree of freedom. Hypothesis test is performed to the actual observation by treating the above Gaussian distribution as the null hypothesis and the judging index as the test statistic. If the null hypothesis should be rejected, it is concluded that outliers exist in the observations. In the presence of outliers scaling factors can be introduced to rescale the covariance of the observation noise or of the innovation vector, both resulting in a decreased ﬁlter gain. And the scaling factors can be solved using the Newton’s iterative method or in an analytical manner. The harmful inﬂuence of either of the two kinds of errors can be effectively resisted in the proposed method, so robustness can be achieved. Moreover, as the number of iterations needed in the iterative method may be rather large, the analytically calculated scaling factor should be preferred.},
	language = {en},
	number = {4},
	urldate = {2021-11-13},
	journal = {Journal of Geodesy},
	author = {Chang, Guobin},
	month = apr,
	year = {2014},
	pages = {391--401},
	file = {Chang - 2014 - Robust Kalman filtering based on Mahalanobis dista.pdf:C\:\\Users\\Florian\\Zotero\\storage\\K3ZI8FKP\\Chang - 2014 - Robust Kalman filtering based on Mahalanobis dista.pdf:application/pdf},
}

@article{li_robust_2021,
	title = {Robust {Variational}-{Based} {Kalman} {Filter} for {Outlier} {Rejection} {With} {Correlated} {Measurements}},
	volume = {69},
	issn = {1941-0476},
	doi = {10.1109/TSP.2020.3042944},
	abstract = {State estimation is a fundamental task in many engineering fields, and therefore robust nonlinear filtering techniques able to cope with misspecified, uncertain and/or corrupted models must be designed for real-life applicability. In this contribution we explore nonlinear Gaussian filtering problems where measurements may be corrupted by outliers, and propose a new robust variational-based filtering methodology able to detect and mitigate their impact. This method generalizes previous contributions to the case of multiple outlier indicators for both independent and dependent observation models. An illustrative example is provided to support the discussion and show the performance improvement.},
	journal = {IEEE Transactions on Signal Processing},
	author = {Li, Haoqing and Medina, Daniel and Vilà-Valls, Jordi and Closas, Pau},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Kalman filters, Bayes methods, correlated measurements, Data models, Estimation, heavy-tailed noise, Noise measurement, outliers, Robust filtering, State-space methods, Task analysis, variational Bayes},
	pages = {357--369},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\UL83WPK5\\Li et al. - 2021 - Robust Variational-Based Kalman Filter for Outlier.pdf:application/pdf},
}

@article{black_eigentracking_1998,
	title = {{EigenTracking}: {Robust} {Matching} and {Tracking} of {Articulated} {Objects} {Using} a {View}-{Based} {Representation}},
	volume = {26},
	issn = {1573-1405},
	shorttitle = {{EigenTracking}},
	url = {https://doi.org/10.1023/A:1007939232436},
	doi = {10.1023/A:1007939232436},
	abstract = {This paper describes an approach for tracking rigid and articulated objects using a view-based representation. The approach builds on and extends work on eigenspace representations, robust estimation techniques, and parameterized optical flow estimation. First, we note that the least-squares image reconstruction of standard eigenspace techniques has a number of problems and we reformulate the reconstruction problem as one of robust estimation. Second we define a “subspace constancy assumption” that allows us to exploit techniques for parameterized optical flow estimation to simultaneously solve for the view of an object and the affine transformation between the eigenspace and the image. To account for large affine transformations between the eigenspace and the image we define a multi-scale eigenspace representation and a coarse-to-fine matching strategy. Finally, we use these techniques to track objects over long image sequences in which the objects simultaneously undergo both affine image motions and changes of view. In particular we use this “EigenTracking” technique to track and recognize the gestures of a moving hand.},
	language = {en},
	number = {1},
	urldate = {2021-11-15},
	journal = {International Journal of Computer Vision},
	author = {Black, Michael J. and Jepson, Allan D.},
	month = jan,
	year = {1998},
	pages = {63--84},
	file = {Springer Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\GW3EEEF3\\Black und Jepson - 1998 - EigenTracking Robust Matching and Tracking of Art.pdf:application/pdf},
}

@inproceedings{wang_improving_2018,
	title = {Improving {Feature}-based {Visual} {SLAM} by {Semantics}},
	doi = {10.1109/IPAS.2018.8708875},
	abstract = {Feature-based simultaneous localization and mapping (SLAM) algorithms with additional semantics can have better feature matching and tracking accuracies than the original SLAM algorithms. Therefore, this paper shows how to improve feature-based SLAM by only matching features from objects of the same semantic class. The basic idea is to use a deep neural network, YOLO (you only look once [1]), to classify objects and to associate features with the objects in whose bounding box they appear, thus giving features the semantic label of these objects. During feature matching of the SLAM algorithms, only features with the same semantic label are matched (e.g. books with books, bottles with bottles etc.), eliminating matches of similar features on different classes of objects. Experiments of classical ORB-SLAM2 with YOLO have been performed on an embedded PC. Additionally, ORB-SLAM2 with different versions of YOLO has also been tested on a powerful desktop GPU as well as on an Nvidia Jetson TX2 board. The experimental results show that using the semantic information given by object recognition methods reduces wrong feature matches in tracking and decreases the tracking lost cases.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Image} {Processing}, {Applications} and {Systems} ({IPAS})},
	author = {Wang, Ya and Zell, Andreas},
	month = dec,
	year = {2018},
	keywords = {Feature extraction, Three-dimensional displays, Image color analysis, neural networks, object recognition, Object recognition, Semantics, simultaneous localization and mapping, Simultaneous localization and mapping, Visualization},
	pages = {7--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\TYIGC7VG\\8708875.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\RCSKVZWG\\Wang und Zell - 2018 - Improving Feature-based Visual SLAM by Semantics.pdf:application/pdf},
}

@inproceedings{chen_improved_2020,
	title = {Improved {ORB}-{SLAM} {Based} {3D} {Dense} {Reconstruction} for {Monocular} {Endoscopic} {Image}},
	doi = {10.1109/ICVRV51359.2020.00030},
	abstract = {Monocular visual simultaneous localization and mapping (SLAM) performs effectively in camera pose estimation and 3D sparse reconstruction of natural scenes. However, in monocular endoscopic environment, serious distortion of the images and the inconstant illumination, even the lack of surface texture, make SLAM-based tracking and 3D dense reconstruction still a challenge. In response to the above problems, it is proposed to use local features to match adjacent frames in ORB-SLAM system for the endoscopic poses estimation and keyframes selection, then combined with the probabilistic monocular stereo technology to calculate the dense depth map from keyframes, and finally complete the 3D dense reconstruction of the endoscopic scene. The experimental results proved that this method can track the endoscope robustly and reconstruct a 3D point cloud with high density and smoothness.},
	booktitle = {2020 {International} {Conference} on {Virtual} {Reality} and {Visualization} ({ICVRV})},
	author = {Chen, Weishan and Liao, Xiangyun and Sun, Yinzi and Wang, Qiong},
	month = nov,
	year = {2020},
	note = {ISSN: 2375-141X},
	keywords = {Three-dimensional displays, Simultaneous localization and mapping, 3D reconstruction, dense depth estimation, Endoscopes, Lighting, monocular endoscope, Pose estimation, Probabilistic logic, Surgery, visual SLAM},
	pages = {101--106},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\KHTTFA98\\Chen et al. - 2020 - Improved ORB-SLAM Based 3D Dense Reconstruction fo.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\3G6XYTM2\\9478624.html:text/html},
}

@article{chen_slam_2019,
	title = {{SLAM} {Endoscopy} enhanced by adversarial depth prediction},
	url = {http://arxiv.org/abs/1907.00283},
	abstract = {Medical endoscopy remains a challenging application for simultaneous localization and mapping (SLAM) due to the sparsity of image features and size constraints that prevent direct depth-sensing. We present a SLAM approach that incorporates depth predictions made by an adversarially-trained convolutional neural network (CNN) applied to monocular endoscopy images. The depth network is trained with synthetic images of a simple colon model, and then fine-tuned with domain-randomized, photorealistic images rendered from computed tomography measurements of human colons. Each image is paired with an error-free depth map for supervised adversarial learning. Monocular RGB images are then fused with corresponding depth predictions, enabling dense reconstruction and mosaicing as an endoscope is advanced through the gastrointestinal tract. Our preliminary results demonstrate that incorporating monocular depth estimation into a SLAM architecture can enable dense reconstruction of endoscopic scenes.},
	urldate = {2021-11-16},
	journal = {arXiv:1907.00283 [cs, eess]},
	author = {Chen, Richard J. and Bobrow, Taylor L. and Athey, Thomas and Mahmood, Faisal and Durr, Nicholas J.},
	month = jun,
	year = {2019},
	note = {arXiv: 1907.00283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\NWY6IVBR\\Chen et al. - 2019 - SLAM Endoscopy enhanced by adversarial depth predi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\XK3JJFQ7\\1907.html:text/html},
}

@article{rodriguez_sd-defslam_2020,
	title = {{SD}-{DefSLAM}: {Semi}-{Direct} {Monocular} {SLAM} for {Deformable} and {Intracorporeal} {Scenes}},
	shorttitle = {{SD}-{DefSLAM}},
	url = {http://arxiv.org/abs/2010.09409},
	abstract = {Conventional SLAM techniques strongly rely on scene rigidity to solve data association, ignoring dynamic parts of the scene. In this work we present Semi-Direct DefSLAM (SD-DefSLAM), a novel monocular deformable SLAM method able to map highly deforming environments, built on top of DefSLAM. To robustly solve data association in challenging deforming scenes, SD-DefSLAM combines direct and indirect methods: an enhanced illumination-invariant Lucas-Kanade tracker for data association, geometric Bundle Adjustment for pose and deformable map estimation, and bag-of-words based on feature descriptors for camera relocation. Dynamic objects are detected and segmented-out using a CNN trained for the specific application domain. We thoroughly evaluate our system in two public datasets. The mandala dataset is a SLAM benchmark with increasingly aggressive deformations. The Hamlyn dataset contains intracorporeal sequences that pose serious real-life challenges beyond deformation like weak texture, specular reflections, surgical tools and occlusions. Our results show that SD-DefSLAM outperforms DefSLAM in point tracking, reconstruction accuracy and scale drift thanks to the improvement in all the data association steps, being the first system able to robustly perform SLAM inside the human body.},
	urldate = {2021-11-16},
	journal = {arXiv:2010.09409 [cs]},
	author = {Rodríguez, Juan J. Gómez and Lamarca, José and Morlana, Javier and Tardós, Juan D. and Montiel, José M. M.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09409},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.5, I.4.6, I.4.8},
	file = {arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\YQ3ZSNIM\\Rodríguez et al. - 2020 - SD-DefSLAM Semi-Direct Monocular SLAM for Deforma.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\4RH6JJF2\\2010.html:text/html},
}

@inproceedings{wei_novel_2020,
	title = {A novel {SLAM} method for laparoscopic scene reconstruction with feature patch tracking},
	doi = {10.1109/ICVRV51359.2020.00071},
	abstract = {As one of the Minimally Invasive Surgery (MIS), laparoscopy is widely applied in gastrointestinal surgery. Benefits from its minimally invasive procedure and fast postoperative rehabilitation for patients. However, it is a huge challenge for surgeons to operate slender surgical instruments under a limited field of view taken by laparoscopy which requires the surgeon to have rich surgical experience. In laparoscopic surgeries, an accurate 3D reconstruction mode with internal anatomy structure and laparoscopic positions in the abdominal cavity can effectively help the surgeon to reduce the dependence on surgical experience. In addition, reconstructing the structure of the surgical scene is also a key step for data registration in surgical navigation and augmented reality surgery. In this paper, a novel Simultaneous Localization and Mapping (SLAM) method based on feature patch tracking by Kernel Correlation Filter (KCF) is proposed for 3D dense point clouds reconstruction, which is called KCF-SLAM. It reconstructs the laparoscopic surgery scene accurately and densely under stereo laparoscopic conditions. The proposed method is validated on a public in-vivo data set captured by a stereo laparoscope. Compared with the feature-based SLAM, the proposed method can work efficiently when the image texture is missing or insignificant. The results suggest that the proposed method can reconstruct the dense point cloud of laparoscopic scene stably and accurately.},
	booktitle = {2020 {International} {Conference} on {Virtual} {Reality} and {Visualization} ({ICVRV})},
	author = {Wei, Guodong and Feng, Guanyuan and Li, Hongliang and Chen, Tao and Shi, Weili and Jiang, Zhengang},
	month = nov,
	year = {2020},
	note = {ISSN: 2375-141X},
	keywords = {Feature extraction, Three-dimensional displays, Simultaneous localization and mapping, Visualization, feature patch, KCF, KCF-SLAM, Laparoscopes, laparoscopic scene reconstruction, Minimally invasive surgery, SLAM, Target tracking},
	pages = {287--291},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\V6ZZM754\\Wei et al. - 2020 - A novel SLAM method for laparoscopic scene reconst.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\TPEWYBNS\\9479817.html:text/html},
}

@article{cavalli_adalam_2020,
	title = {{AdaLAM}: {Revisiting} {Handcrafted} {Outlier} {Detection}},
	shorttitle = {{AdaLAM}},
	url = {http://arxiv.org/abs/2006.04250},
	abstract = {Local feature matching is a critical component of many computer vision pipelines, including among others Structure-from-Motion, SLAM, and Visual Localization. However, due to limitations in the descriptors, raw matches are often contaminated by a majority of outliers. As a result, outlier detection is a fundamental problem in computer vision, and a wide range of approaches have been proposed over the last decades. In this paper we revisit handcrafted approaches to outlier filtering. Based on best practices, we propose a hierarchical pipeline for effective outlier detection as well as integrate novel ideas which in sum lead to AdaLAM, an efficient and competitive approach to outlier rejection. AdaLAM is designed to effectively exploit modern parallel hardware, resulting in a very fast, yet very accurate, outlier filter. We validate AdaLAM on multiple large and diverse datasets, and we submit to the Image Matching Challenge (CVPR2020), obtaining competitive results with simple baseline descriptors. We show that AdaLAM is more than competitive to current state of the art, both in terms of efficiency and effectiveness.},
	urldate = {2021-11-16},
	journal = {arXiv:2006.04250 [cs]},
	author = {Cavalli, Luca and Larsson, Viktor and Oswald, Martin Ralf and Sattler, Torsten and Pollefeys, Marc},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04250},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\3Y9AARNJ\\Cavalli et al. - 2020 - AdaLAM Revisiting Handcrafted Outlier Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\LLUQT4QM\\2006.html:text/html},
}

@article{li_research_2021,
	title = {Research and improvement of feature detection algorithm based on {FAST}},
	volume = {32},
	issn = {1720-0776},
	url = {https://doi.org/10.1007/s12210-021-01020-1},
	doi = {10.1007/s12210-021-01020-1},
	abstract = {At present, in the application of feature-based medical 3D reconstruction technology, there are still problems such as low matching accuracy of feature points in endoscope images and slow processing speed of image data. Therefore, the feature-based 3D reconstruction theory is of great research value and has great application value. This paper proposed a new feature detection method to improve the problems. This paper divides feature detection into two parts for further improvements: feature extraction and feature description. For feature extraction, the FAST algorithm shows a poor classification effect, so this paper adds the decision tree based on the C4.5 algorithm into the traditional FAST. The original data are divided into two decision trees to make the feature extraction performance more stable and feature point extraction more efficient. For the feature description part, the FREAK descriptor is used, combined with this paper's improved feature extraction algorithm. The feature points are extracted in scale space. The second-order function fitting is carried out according to the feature points' response scores in different scales. The scale-invariant descriptor of sub-pixel precision is obtained. The experimental results on the endoscope image show that the feature extraction method has a higher extraction accuracy and faster extraction speed. In addition, the feature description algorithm has higher calculation efficiency.},
	language = {en},
	number = {4},
	urldate = {2021-11-25},
	journal = {Rendiconti Lincei. Scienze Fisiche e Naturali},
	author = {Li, Yulin and Zheng, Wenfeng and Liu, Xiangjun and Mou, Yuanyuan and Yin, Lirong and Yang, Bo},
	month = dec,
	year = {2021},
	pages = {775--789},
	file = {Springer Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\AQ73SCNM\\Li et al. - 2021 - Research and improvement of feature detection algo.pdf:application/pdf},
}

@inproceedings{fan_3d_2010,
	title = {{3D} reconstruction of wireless capsule endoscopy images},
	doi = {10.1109/IEMBS.2010.5626182},
	abstract = {Wireless capsule endoscopy (WCE) has been gradually applied for inspecting the gastrointestinal (GI) tract. However, WCE can only provide monocular view. Moreover, only a small part of GI wall is visible frame by frame due to the limited illumination and irregular motion of the capsule endoscope. The perception of entire GI structure could be hard even for the experienced endoscopists. A realistic friendly three dimension view is needed to help the physicians to get a better perception of the GI tract. In this paper, we present a method to reconstruct the three dimension surface of the intestinal wall by applying the SIFT feature detector and descriptor to a sequence of WCE images. Epipolar geometry is employed to further constrain the matching feature points in order to obtain a more accurate 3D view. The experiments on real data are presented to show the performance of our proposed method.},
	booktitle = {2010 {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology}},
	author = {Fan, Yichen and Meng, Max Q.-H. and Li, Baopu},
	month = aug,
	year = {2010},
	note = {ISSN: 1558-4615},
	keywords = {Feature extraction, Endoscopes, Geometry, Image reconstruction, Medical services, Surface reconstruction, Three dimensional displays},
	pages = {5149--5152},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\XXSEIGPM\\Fan et al. - 2010 - 3D reconstruction of wireless capsule endoscopy im.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\8V6B9V2A\\5626182.html:text/html},
}

@article{lin_efficient_2015,
	title = {Efficient {Vessel} {Feature} {Detection} for {Endoscopic} {Image} {Analysis}},
	volume = {62},
	issn = {1558-2531},
	doi = {10.1109/TBME.2014.2373273},
	abstract = {Distinctive feature detection is an essential task in computer-assisted minimally invasive surgery (MIS). For special conditions in an MIS imaging environment, such as specular reflections and texture homogeneous areas, the feature points extracted by general feature point detectors are less distinctive and repeatable in MIS images. We observe that abundant blood vessels are available on tissue surfaces and can be extracted as a new set of image features. In this paper, two types of blood vessel features are proposed for endoscopic images: branching points and branching segments. Two novel methods, ridgeness-based circle test and ridgeness-based branching segment detection are presented to extract branching points and branching segments, respectively. Extensive in vivo experiments were conducted to evaluate the performance of the proposed methods and compare them with the state-of-the-art methods. The numerical results verify that, in MIS images, the blood vessel features can produce a large number of points. More importantly, those points are more robust and repeatable than the other types of feature points. In addition, due to the difference in feature types, vessel features can be combined with other general features, which makes them new tools for MIS image analysis. These proposed methods are efficient and the code and datasets are made available to the public.},
	number = {4},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Lin, Bingxiong and Sun, Yu and Sanchez, Jaime E. and Qian, Xiaoning},
	month = apr,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Biomedical Engineering},
	keywords = {Feature extraction, Robustness, Biomedical imaging, Blood vessels, Branching point, branching segment, Detectors, Eigenvalues and eigenfunctions, endoscopic image analysis, Image segmentation, minimally invasive surgery (MIS), vessel feature detection},
	pages = {1141--1150},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\PPS4TPM4\\Lin et al. - 2015 - Efficient Vessel Feature Detection for Endoscopic .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Florian\\Zotero\\storage\\HMWP52NL\\6963409.html:text/html},
}

@inproceedings{detone_superpoint_2018,
	title = {{SuperPoint}: {Self}-{Supervised} {Interest} {Point} {Detection} and {Description}},
	shorttitle = {{SuperPoint}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018_workshops/w9/html/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.html},
	urldate = {2021-12-06},
	author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	year = {2018},
	pages = {224--236},
	file = {Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\63HYKZ6T\\DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.html:text/html;Full Text PDF:C\:\\Users\\Florian\\Zotero\\storage\\8UQJUP2Y\\DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detecti.pdf:application/pdf},
}

@article{tang_neural_2019,
	title = {Neural {Outlier} {Rejection} for {Self}-{Supervised} {Keypoint} {Learning}},
	url = {http://arxiv.org/abs/1912.10615},
	abstract = {Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks. However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.},
	urldate = {2021-12-06},
	journal = {arXiv:1912.10615 [cs]},
	author = {Tang, Jiexiong and Kim, Hanme and Guizilini, Vitor and Pillai, Sudeep and Ambrus, Rares},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.10615},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\N5FMPLJB\\1912.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\UMNBXIKF\\Tang et al. - 2019 - Neural Outlier Rejection for Self-Supervised Keypo.pdf:application/pdf},
}

@article{sarlin_superglue_2020,
	title = {{SuperGlue}: {Learning} {Feature} {Matching} with {Graph} {Neural} {Networks}},
	shorttitle = {{SuperGlue}},
	url = {http://arxiv.org/abs/1911.11763},
	abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
	urldate = {2021-12-06},
	journal = {arXiv:1911.11763 [cs]},
	author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.11763},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\2IUQ8XDI\\1911.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\DQBA4B3H\\Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf:application/pdf},
}

@article{detone_superpoint_2018-1,
	title = {{SuperPoint}: {Self}-{Supervised} {Interest} {Point} {Detection} and {Description}},
	shorttitle = {{SuperPoint}},
	url = {http://arxiv.org/abs/1712.07629},
	abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
	urldate = {2021-12-06},
	journal = {arXiv:1712.07629 [cs]},
	author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	month = apr,
	year = {2018},
	note = {arXiv: 1712.07629},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Florian\\Zotero\\storage\\6RPJQ5UD\\1712.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Florian\\Zotero\\storage\\8EA8JUNR\\DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detecti.pdf:application/pdf},
}
